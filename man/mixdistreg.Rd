% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/main_mix.R
\name{mixdistreg}
\alias{mixdistreg}
\title{Generic mixture distributional regression}
\usage{
mixdistreg(
  y,
  families = c("normal"),
  type = c("same", "general"),
  nr_comps = 2L,
  list_of_formulas,
  formula_mixture = ~1,
  list_of_deep_models = NULL,
  inflation_values = NULL,
  data,
  trafos_each_param = NULL,
  entropy_penalty = NULL,
  ...
)
}
\arguments{
\item{y}{response}

\item{families}{character (vector); 
see the \code{family} argument of \code{deepregression}. Can be multiple
distributions for a mixture of different distributions (\code{type = "general"})}

\item{type}{character; either \code{"same"} if mixture of same families or 
\code{"general"} for a general mixture}

\item{nr_comps}{integer; number of mixture components}

\item{list_of_formulas}{a list of formulas for the 
parameters of the distribution(s) which are used in the mixture
(i.e., for \code{family="normal"} a list of two formulas
is required). The elements in the list first consist of all parameters for
the first distribution, then all for the second distribution and so on.
See also \code{?deepregression} for specification details for the formulas.}

\item{formula_mixture}{formula for the the additive predictor 
of the mixture component. Covariate-independent per default.}

\item{list_of_deep_models}{see \code{?deepregression}}

\item{inflation_values}{see \code{?inflareg}}

\item{data}{data.frame or list with data}

\item{trafos_each_param}{transformation for each parameter; see
\code{?gen_mix_dist_maker}.}

\item{entropy_penalty}{numeric; if not NULL, will add an entrop penalty
to the negative log-likelihood to penalize the amount of mixtures}

\item{...}{further arguments passed to \code{?deepregression}}
}
\value{
a model of class \code{mixdistreg} and
\code{deepregression}
}
\description{
Fit a mixture distributional regression
}
\examples{

n <- 1000
data = data.frame(matrix(rnorm(4*n), c(n,4)))
colnames(data) <- c("x1","x2","x3","xa")
formula <- ~ 1 + deep_model(x1,x2,x3) + s(xa) + x1

deep_model <- function(x) x \%>\%
layer_dense(units = 32, activation = "relu", use_bias = FALSE) \%>\%
layer_dropout(rate = 0.2) \%>\%
layer_dense(units = 8, activation = "relu") \%>\%
layer_dense(units = 1, activation = "linear")

y <- rnorm(n) + data$xa^2 + data$x1

mod <- mixdistreg(
  families = c("normal", "student_t"),
  type = "general",
  list_of_formulas = list(
    loc = formula, scale = ~ 1, 
    df = formula
    ),
   
  formula_mixture = ~ 1 + x1,
  data = data, 
  y = y,
  list_of_deep_models = list(deep_model = deep_model),
  inflation_values = NULL,
  optimizer = optimizer_adam(learning_rate=1e-6)
)

if(!is.null(mod)){

# train for more than 10 epochs to get a better model
mod \%>\% fit(epochs = 10, early_stopping = TRUE)

}


}
