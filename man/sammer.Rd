% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/main_mix.R
\name{sammer}
\alias{sammer}
\title{Same-same mixture distributional regression}
\usage{
sammer(
  y,
  family = "normal",
  nr_comps = 2L,
  list_of_formulas,
  formula_mixture = ~1,
  list_of_deep_models = NULL,
  data,
  ...
)
}
\arguments{
\item{y}{response}

\item{family}{character; see \code{?deepregression}}

\item{nr_comps}{integer; number of mixture components}

\item{list_of_formulas}{a list of formulas for the 
parameters of the distribution which is used in the mixture
(i.e., for \code{family="normal"} only a list of two formulas
is required). See also \code{?deepregression} for more details.}

\item{formula_mixture}{formula for the the additive predictor 
of the mixture component. Covariate-independent per default.}

\item{list_of_deep_models}{see \code{?deepregression}}

\item{data}{data.frame or list with data}

\item{...}{further arguments passed to \code{?deepregression}}
}
\value{
a model of class \code{mixdistreg} and
\code{deepregression}
}
\description{
Fit a mixture distributional regression with same family mixture
and same predictors for each mixture
}
\examples{
n <- 1000
data = data.frame(matrix(rnorm(4*n), c(n,4)))
colnames(data) <- c("x1","x2","x3","xa")
formula <- ~ 1 + deep_model(x1,x2,x3) + s(xa) + x1

deep_model <- function(x) x \%>\%
layer_dense(units = 32, activation = "relu", use_bias = FALSE) \%>\%
layer_dropout(rate = 0.2) \%>\%
layer_dense(units = 8, activation = "relu") \%>\%
layer_dense(units = 1, activation = "linear")

y <- rnorm(n) + data$xa^2 + data$x1

mod <- sammer(
  list_of_formulas = list(loc = formula, scale = ~ 1),
  nr_comps = 3,
  data = data, y = y,
  list_of_deep_models = list(deep_model = deep_model)
)

if(!is.null(mod)){

# train for more than 10 epochs to get a better model
mod \%>\% fit(epochs = 10, early_stopping = TRUE)

}

}
